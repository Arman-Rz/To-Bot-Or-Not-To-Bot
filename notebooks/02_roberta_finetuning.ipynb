{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b6eb142-067f-4e8d-9f1b-b3fe6ca29996",
   "metadata": {},
   "source": [
    "# RoBERTa Finetuning\n",
    "In this notebook we will:\n",
    "\n",
    "1. Load the cleaned users.csv and posts.csv generated by the previous notebook.\n",
    "2. Buid a user-level document = bio/description + tweets.\n",
    "3. Split users into **train** / **validation** / **test** (stratified by label)\n",
    "4. Fine-Tune **RoBERTa-base** on GPU.\n",
    "5. Evaluate (accuracy, macro F1, AUROC)\n",
    "6. Save the model and metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a15003-8c46-4705-8996-d8c23881077c",
   "metadata": {},
   "source": [
    "# Section 1: Imports and Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82534cf6-11f8-4b09-8673-26d92f742865",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------------#\n",
    "# HuggingFace Cache Location                                                      #\n",
    "#---------------------------------------------------------------------------------#\n",
    "# By default, HuggingFace downloads pretrained models into the user directory     #\n",
    "# (e.g., ~/.cache/huggingface/). To make the project fully reproducible and       #\n",
    "# avoid polluting the user's  global cache, we redirect HF_HOME to a local        #\n",
    "# folder inside the project.                                                      #\n",
    "#                                                                                 #\n",
    "# If you prefer a different cache directory, simply modify HF_CACHE below.        #\n",
    "# If the folder does not exist yet, HuggingFace will create it automatically.     #\n",
    "#---------------------------------------------------------------------------------#\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path.cwd().parent.resolve()\n",
    "HF_CACHE = ROOT / \"hf_cache\"\n",
    "os.environ[\"HF_HOME\"] = str(HF_CACHE)\n",
    "\n",
    "import random\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from transformers import (\n",
    "AutoTokenizer,\n",
    "AutoModelForSequenceClassification,\n",
    "TrainingArguments,\n",
    "Trainer,\n",
    "DataCollatorWithPadding\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --- Paths and Configurations --- #\n",
    "DATA_DIR   = ROOT / \"data/twibot22/processed\"\n",
    "OUTPUT_DIR = ROOT / \"outputs/roberta_twibot22\"\n",
    "\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MODEL_NAME = \"roberta-base\"\n",
    "\n",
    "SEED           = 42\n",
    "MAX_SEQ_LENGTH = 256\n",
    "BATCH_SIZE     = 4\n",
    "NUM_EPOCHS     = 4\n",
    "LEARNING_RATE  = 2e-5\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device.type == \"cuda\":\n",
    "    print(f\"Using cuda on {torch.cuda.get_device_name()}\")\n",
    "else:\n",
    "    print(\"Using cpu\")\n",
    "\n",
    "def space():\n",
    "    print(\"\\n\" ,\"-\" * 100, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2518db95-7897-4ccb-865d-16498d362590",
   "metadata": {},
   "source": [
    "# Section 2: Loading cleaned Twibot-22 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb48f4d-8d7b-4058-a9d5-ef59a538db16",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_path = DATA_DIR / \"users.csv\"\n",
    "posts_path = DATA_DIR / \"posts.csv\"\n",
    "\n",
    "users_df = pd.read_csv(str(users_path))\n",
    "posts_df = pd.read_csv(str(posts_path))\n",
    "\n",
    "print(\"Users shape:\", users_df.shape)\n",
    "print(\"Posts shape:\", posts_df.shape)\n",
    "\n",
    "space()\n",
    "    \n",
    "print(\"User label distribution (0: Human, 1: Bot):\")\n",
    "print(users_df['label'].value_counts())\n",
    "\n",
    "display(users_df.head())\n",
    "display(posts_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c67e9b-f255-42dd-b29b-426db4eb4097",
   "metadata": {},
   "source": [
    "# Section 3: Building user-level documents\n",
    "We create a single text per user: bio/description + concatenated tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270588ec-7e20-4a1d-b88b-c00888e89fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Aggregating tweets per user (concatenated text) --- #\n",
    "posts_agg = (\n",
    "    posts_df.groupby(\"id\")[\"text\"]\n",
    "    .apply(lambda x: \" \".join(x.astype(str)))\n",
    "    .reset_index()\n",
    "    .rename(columns={\"text\": \"tweets_text\"})\n",
    ")\n",
    "\n",
    "# --- Merging into users --- #\n",
    "data_df = users_df.merge(posts_agg, on=\"id\", how=\"left\")\n",
    "data_df[\"tweets_text\"] = data_df[\"tweets_text\"].fillna(\"\")\n",
    "\n",
    "# --- Explicit Bio text --- #\n",
    "data_df[\"bio_text\"] = data_df[\"description\"].fillna(\"\")\n",
    "\n",
    "MAX_TWEET_CHARS = 5000\n",
    "data_df[\"tweets_text\"] = data_df[\"tweets_text\"].str.slice(0, MAX_TWEET_CHARS)\n",
    "\n",
    "# --- Full text = description + tweets --- #\n",
    "data_df[\"full_text\"] = (\n",
    "    \"Bio: \" + data_df[\"bio_text\"] + \" Posts: \" + data_df[\"tweets_text\"]\n",
    ").str.strip()\n",
    "\n",
    "# --- Length stats --- #\n",
    "data_df[\"char_length\"] = data_df[\"full_text\"].apply(len)\n",
    "print(\"Doc length summary (chars):\")\n",
    "display(data_df[\"char_length\"].describe())\n",
    "\n",
    "space()\n",
    "\n",
    "# --- Filtering out users with every little text --- #\n",
    "min_char = 50\n",
    "data_df = data_df[data_df[\"char_length\"] >= min_char].reset_index(drop=True)\n",
    "print(\"Users after min_char filter:\", len(data_df))\n",
    "print(data_df[\"label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d544cbb2-3f80-4c3d-b2ae-f554fe1851e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "humans = data_df[data_df[\"label_num\"] == 0]\n",
    "bots = data_df[data_df[\"label_num\"] == 1]\n",
    "\n",
    "n = min(len(humans), len(bots))\n",
    "humans_bal = humans.sample(n, random_state=SEED)\n",
    "bots_bal = bots.sample(n, random_state=SEED)\n",
    "\n",
    "data_bal = pd.concat([humans_bal, bots_bal]).sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "print(\"Users after balancing:\", len(data_bal))\n",
    "print(data_bal[\"label\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b3a74e-439a-4c36-8a46-a2d692a5190e",
   "metadata": {},
   "source": [
    "# Section 4: Train / Val / Test split (user-level, stratified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675180e2-aa5a-4da1-b7df-6a9501a48981",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_df, test_df = train_test_split(\n",
    "    data_bal,\n",
    "    test_size=0.2,\n",
    "    stratify=data_bal[\"label_num\"],\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "train_df, val_df = train_test_split(\n",
    "    train_val_df,\n",
    "    test_size=0.2,\n",
    "    stratify=train_val_df[\"label_num\"],\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "print(\"Train users:\", len(train_df))\n",
    "print(\"Val users:\", len(val_df))\n",
    "print(\"Test users:\", len(test_df))\n",
    "\n",
    "space()\n",
    "\n",
    "print(\"\\nTrain label dist:\")\n",
    "print(train_df[\"label\"].value_counts())\n",
    "\n",
    "space()\n",
    "\n",
    "print(\"\\nVal label dist:\")\n",
    "print(val_df[\"label\"].value_counts())\n",
    "\n",
    "space()\n",
    "\n",
    "print(\"\\nTest label dist:\")\n",
    "print(test_df[\"label\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ebbb5f-2b57-45a3-82f6-6a5a3a0e72ae",
   "metadata": {},
   "source": [
    "# Section 5: Preparing tokenizer and dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d32310-01ba-4e06-8da3-a92f7da5d470",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "class UserTextDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len):\n",
    "        self.texts     = df[\"full_text\"].tolist()\n",
    "        self.labels    = df[\"label_num\"].astype(int).tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len   = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text  = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        enc   = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=False,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        # enc[\"input_ids\"] has shape [1, seq_len], we squeeze later in collator\n",
    "        item = {k: v.squeeze(0) for k, v in enc.items()}\n",
    "        item[\"labels\"] = torch.tensor(label, dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "train_dataset = UserTextDataset(train_df, tokenizer, MAX_SEQ_LENGTH)\n",
    "val_dataset   = UserTextDataset(val_df,   tokenizer, MAX_SEQ_LENGTH)\n",
    "test_dataset  = UserTextDataset(test_df,  tokenizer, MAX_SEQ_LENGTH)\n",
    "\n",
    "print(\"Train dataset size:\", len(train_dataset))\n",
    "print(\"Val dataset size:\",   len(val_dataset))\n",
    "print(\"Test dataset size:\",  len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8218929-c3bc-45d9-9bb3-9dfe547c0fe0",
   "metadata": {},
   "source": [
    "# Section 6: Loading RoBERTa model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03e746e-72de-4244-8db8-64b9bda89c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = 2\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=num_labels\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "print(\"Model loaded on:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b79ba19-ca29-4b53-9b14-f431c17b03eb",
   "metadata": {},
   "source": [
    "# Section 7: Training Arguments and Trainer\n",
    "We use **HuggingFace's Trainer** to simplify the fine-tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c44d59-bb76-4a4f-96e4-98ed9f277a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=str(OUTPUT_DIR / \"checkpoints\"),\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    logging_steps=50,\n",
    "    save_total_limit=2,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=[]\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=1)\n",
    "    report = classification_report(labels, preds, output_dict=True, zero_division=0)\n",
    "    macro_f1 = report[\"macro avg\"][\"f1-score\"]\n",
    "    accuracy = report[\"accuracy\"]\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"macro_f1\": macro_f1\n",
    "    }\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f12ebe2-3bc7-4b5c-8247-71a03cc6e4e9",
   "metadata": {},
   "source": [
    "# Section 8: Fine-tuning RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb53c0d-acfb-4f26-a321-273341ff40b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "print(\"Starting to train (might take a bit)...\")\n",
    "train_result = trainer.train()\n",
    "trainer.save_model(str(OUTPUT_DIR / \"roberta_twibot22_model\"))\n",
    "tokenizer.save_pretrained(str(OUTPUT_DIR / \"roberta_twibot22_model\"))\n",
    "\n",
    "print(\"Training finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d17e68a-050d-4c34-881d-1f7a9472986d",
   "metadata": {},
   "source": [
    "# Section 9: Evaluation on validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9011a4-09f0-4ded-ae77-439b4e810cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_preds = trainer.predict(val_dataset)\n",
    "val_logits = val_preds.predictions\n",
    "val_labels = val_preds.label_ids\n",
    "\n",
    "val_probs = torch.softmax(torch.tensor(val_logits), dim=-1).numpy()[:, 1]\n",
    "\n",
    "print(\"Validation labels shape:\", val_labels.shape)\n",
    "print(\"Validation probs shape:\", val_probs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f5654b-a7c5-4723-84fd-7cc41031a944",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = np.linspace(0.1, 0.9, 17)\n",
    "\n",
    "best_thr = 0.5\n",
    "best_macro_f1 = -1\n",
    "results = []\n",
    "\n",
    "for thr in thresholds:\n",
    "    val_pred_label_thr = (val_probs >= thr).astype(int)\n",
    "    report = classification_report(\n",
    "        val_labels, val_pred_label_thr, output_dict=True, zero_division=0\n",
    "    )\n",
    "    macro_f1 = report[\"macro avg\"][\"f1-score\"]\n",
    "    bot_recall = report[\"1\"][\"recall\"]\n",
    "    results.append((thr, macro_f1, bot_recall))\n",
    "    print(f\"thr={thr:.2f} | macro F1={macro_f1:.3f} | bot recall={bot_recall:.3f}\")\n",
    "\n",
    "    if macro_f1 > best_macro_f1:\n",
    "        best_macro_f1 = macro_f1\n",
    "        best_thr = thr\n",
    "\n",
    "print(\"\\nBest threshold on validation:\")\n",
    "print(f\"thr={best_thr:.2f} with macro F1={best_macro_f1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a642c8-3656-4bf0-8e0a-0ae4bc79527a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Evaluating on validation set --- #\n",
    "print(\"Validation set evaluation:\")\n",
    "val_metrics = trainer.evaluate(val_dataset)\n",
    "print(val_metrics)\n",
    "\n",
    "# Predicting on test set --- #\n",
    "test_preds = trainer.predict(test_dataset)\n",
    "test_logits = test_preds.predictions\n",
    "test_labels = test_preds.label_ids\n",
    "test_probs = torch.softmax(torch.tensor(test_logits), dim=-1).numpy()[:, 1]\n",
    "\n",
    "thr = best_thr\n",
    "test_pred_labels_thr = (test_probs >= thr).astype(int)\n",
    "\n",
    "print(\"Test set classification report (threshold):\")\n",
    "print(classification_report(test_labels, test_pred_labels_thr, digits=3, zero_division=0))\n",
    "\n",
    "\n",
    "try:\n",
    "    auroc = roc_auc_score(test_labels, test_probs)\n",
    "    print(\"Test AUROC:\", auroc)\n",
    "except Exception as e:\n",
    "    print(\"Could not compute AUROC:\", e)\n",
    "\n",
    "cm = confusion_matrix(test_labels, test_pred_labels_thr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340b0208-4443-412f-99f9-c9f977b74039",
   "metadata": {},
   "source": [
    "# Section 10: Confusion matrix and save metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef4accb-a59a-437f-b396-1fedcd8aa75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5, 4))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "           xticklabels=[\"human\", \"bot\"],\n",
    "           yticklabels=[\"human\",\"bot\"])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"RoBERTa Twibot-22 - Confusion Matrix (Test)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(str(OUTPUT_DIR / \"confusion_matrix_test.png\"))\n",
    "plt.show()\n",
    "\n",
    "# --- Saving metrics to JSON/text for the REPORT\n",
    "import json\n",
    "\n",
    "metrics_out = {\n",
    "    \"val_metrics\": val_metrics,\n",
    "    \"test_report\": classification_report(test_labels, test_pred_labels_thr, digits=3, zero_division=0, output_dict=True),\n",
    "    \"test_auroc\": float(roc_auc_score(test_labels, test_probs)) if len(np.unique(test_labels)) == 2 else None \n",
    "}\n",
    "with open(str(OUTPUT_DIR / \"metrics.json\"), \"w\") as f:\n",
    "    json.dump(metrics_out, f, indent=2)\n",
    "\n",
    "print(\"Metrics saved to:\", OUTPUT_DIR / \"metrics.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
