{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25a099ab-86be-44c9-979c-a775ba05f384",
   "metadata": {},
   "source": [
    "# TwiBot-22 Data Cleaning\n",
    "In this notebook, we explore the TwiBot-22 dataset and extracting clean CSVs for our model training. TwiBot-22 is the largest and most comprehensive Twitter dataset to date. Due to the files being very large, we need to take a different action to read and extract the desired data without overwhelming the hardware.  \n",
    "For this project, we only need three files form the dataset. `users`, `labels` and one of the `tweet`s files. After downloading, we make sure they are named:\n",
    "- user.json\n",
    "- label.json\n",
    "- tweet.json\n",
    " \n",
    "Then, we follow these procedures:\n",
    "\n",
    "1. Reading and loading the labels.\n",
    "2. Reading and loading the users's metadata.\n",
    "3. Merging users with labels and filtering out unwanted columns.\n",
    "4. Straming tweets using `ijson` to read the data item by item. This way, we will not need to open the entire file on the device and overload the memory.\n",
    "5. Building `posts` dataframe by matching users and their tweets, and only keep users with a selected minimum tweets per user.\n",
    "6. Downloading the profile image of the users that we kept in the previuous step and updating the users dataframe with the local direction of their profile image.\n",
    "7. Exporting `users.csv` and `posts.csv`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce96835-714c-449d-bf0c-807f32ff9041",
   "metadata": {},
   "source": [
    "## Section 1: Imports & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34788d3c-192e-4c97-a395-7c16630f362e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from tqdm.auto import tqdm\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Paths and Configurations --- #\n",
    "ROOT = Path.cwd().parent.resolve()\n",
    "DATA_DIR = ROOT / \"data/twibot22\"\n",
    "PROCESSED_DIR = DATA_DIR / \"processed\"\n",
    "IMAGE_DIR = DATA_DIR / \"profile_images\"\n",
    "\n",
    "# --- Downloaded files direction (rename file names if needed)\n",
    "LABEL_PATH = DATA_DIR / \"label.csv\"\n",
    "USER_PATH  = DATA_DIR / \"user.json\"\n",
    "TWEET_PATH = DATA_DIR / \"tweet.json\"\n",
    "\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "IMAGE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Balancing & Sampling --- #\n",
    "TARGET_USER_PER_CLASS = 2000   # Humans and Bots each\n",
    "MAX_TWEETS_PER_USER   = 50     # Cap per user to keep dataset managable\n",
    "MIN_TWEETS_PER_USER   = 5      # Users with fewer than this will be dropped\n",
    "\n",
    "print(\"Label path:    \", LABEL_PATH)\n",
    "print(\"User json:     \", USER_PATH)\n",
    "print(\"Tweet json:    \", TWEET_PATH)\n",
    "print(\"Processed dir: \", PROCESSED_DIR)\n",
    "\n",
    "def space():\n",
    "    print(\"\\n\" ,\"-\" * 100, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78598f81-d074-4678-b760-7d706fa47951",
   "metadata": {},
   "source": [
    "## Section 2: Loading labels (human vs bot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49d3239-a1e3-4f2e-a0e0-a44ffe320040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Loading labels --- #\n",
    "labels_df = pd.read_csv(LABEL_PATH)\n",
    "\n",
    "print(\"Raw label head:\")\n",
    "display(labels_df.head())\n",
    "\n",
    "space()\n",
    "\n",
    "print(\"\\nLabel value counts:\")\n",
    "print(labels_df[\"label\"].value_counts())\n",
    "\n",
    "# --- Mapping 'human' -> 0, 'bot' -> 1 (ignoring others if present) --- #\n",
    "valid_labels = [\"human\", \"bot\"]\n",
    "labels_df = labels_df[labels_df[\"label\"].isin(valid_labels)].copy()\n",
    "\n",
    "label_map = {\"human\": 0, \"bot\": 1}\n",
    "labels_df[\"label_num\"] = labels_df[\"label\"].map(label_map)\n",
    "\n",
    "space()\n",
    "\n",
    "print(\"\\nAfter filtering to {human, bot}:\")\n",
    "print(labels_df[\"label\"].value_counts())\n",
    "print(\"\\nTotal labled users:\", len(labels_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36014618-662a-42f2-9243-c05c5c552a48",
   "metadata": {},
   "source": [
    "## Section 3: Loading user metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d99836-84d4-4535-bd52-3f0140c61fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Loading user metadata ---#\n",
    "print(\"Loading user.json (this might take a bit)...\", end=\"\\r\")\n",
    "\n",
    "with open(str(USER_PATH), 'r', encoding='utf-8') as f:\n",
    "    users_raw = json.load(f)    # users.json is a JSON array: [ {...}, {...}, ... ]\n",
    "\n",
    "print(f\"Total users in user.json: {len(users_raw)}           \")\n",
    "\n",
    "# --- Converting to DataFrame ---#\n",
    "users_df = pd.DataFrame(users_raw)\n",
    "\n",
    "print(\"\\nUser keys:\", users_df.columns.tolist()[:30])\n",
    "\n",
    "space()\n",
    "\n",
    "display(users_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61433457-f17b-4938-9eac-ec52e53fbd20",
   "metadata": {},
   "source": [
    "## Section 4: Merging labels with users\n",
    "Here, we only keep users who both exist in `user.json` and have a label in `label.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac38f3d2-094e-4a96-bdc9-e993a47ac2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Merging labels & users --- #\n",
    "# labels_df: columns [id, label, label_num]\n",
    "# users_df:  columns [id, username, description, ...]\n",
    "\n",
    "merged_users = labels_df.merge(users_df, on=\"id\", how=\"inner\")\n",
    "\n",
    "print(\"Users with both metadata and label:\", len(merged_users))\n",
    "print(merged_users['label'].value_counts())\n",
    "\n",
    "space()\n",
    "\n",
    "# --- Keeping only a few user fields --- #\n",
    "keep_cols = [\"id\", \"label\", \"label_num\", \"username\", \"description\", \"created_at\", \"profile_image_url\"]\n",
    "keep_cols = [c for c in keep_cols if c in merged_users.columns]    # Just to be safe\n",
    "merged_users = merged_users[keep_cols].copy()\n",
    "\n",
    "print(\"\\nMerged user sample:\")\n",
    "display(merged_users.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b48243c-f673-4ae0-9c9c-aa18f0a88dff",
   "metadata": {},
   "source": [
    "## Section 5: Selecting balanced user subset\n",
    "We balance **by user**, not by tweet.  \n",
    "We pick up to **2000 humans** and **2000 bots** (or less, if the dataset has fewer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ebff09-dd44-4bb8-85cf-7e664c9937a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Selecting balanced user subset --- #\n",
    "humans = merged_users[merged_users[\"label_num\"] == 0]\n",
    "bots   = merged_users[merged_users[\"label_num\"] == 1]\n",
    "\n",
    "print(\"Available human users:\", len(humans))\n",
    "print(\"Available bot users:\", len(bots))\n",
    "\n",
    "space()\n",
    "\n",
    "humans_num = min(len(humans), TARGET_USER_PER_CLASS)\n",
    "bots_num   = min(len(bots), TARGET_USER_PER_CLASS)\n",
    "\n",
    "print(f\"Sampling {humans_num} humans and {bots_num} bots\")\n",
    "\n",
    "humans_sample = humans.sample(humans_num, random_state=42) if humans_num > 0 else humans\n",
    "bots_sample = bots.sample(bots_num, random_state=42) if bots_num > 0 else bots\n",
    "\n",
    "selected_users = pd.concat([humans_sample, bots_sample]).sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "space()\n",
    "\n",
    "print(\"Selected users:\", len(selected_users))\n",
    "print(selected_users['label'].value_counts())\n",
    "\n",
    "selected_user_ids = set(selected_users['id'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a4272d-26c3-44fd-868a-673751c0974a",
   "metadata": {},
   "source": [
    "## Section 6: Stream tweets from tweet.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdac5236-303f-46f4-9619-fe15684b9c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Tweet streaming helper for json array --- #\n",
    "import ijson\n",
    "\n",
    "def iter_tweet_array(path):\n",
    "    \"\"\"\n",
    "    Iterates tweets from a large json file.\n",
    "    We use ijson to stream items without loading the entire file.\n",
    "    \"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for tweet in ijson.items(f, \"item\"):\n",
    "            if tweet is None:\n",
    "                continue\n",
    "            yield tweet\n",
    "\n",
    "# --- Using helper to stream tweets --- #\n",
    "tweets_records = []\n",
    "tweet_counts = defaultdict(int)\n",
    "\n",
    "total_lines = 0\n",
    "kept_tweets = 0\n",
    "\n",
    "print(f\"Streaming tweets from:\", TWEET_PATH)\n",
    "print(f\"Target max tweet per user: {MAX_TWEETS_PER_USER}\")\n",
    "\n",
    "for tweet in iter_tweet_array(TWEET_PATH):\n",
    "    total_lines += 1\n",
    "\n",
    "    author_id = 'u' + str(tweet.get(\"author_id\"))  # To match the id in users and labels files.\n",
    "    text = tweet.get(\"text\")\n",
    "\n",
    "    if author_id is None or text is None:\n",
    "        continue\n",
    "\n",
    "    if author_id not in selected_user_ids:\n",
    "        continue\n",
    "\n",
    "    if tweet_counts[author_id] >= MAX_TWEETS_PER_USER:\n",
    "        # Already reached per-user cap\n",
    "        continue\n",
    "\n",
    "    tweets_records.append({\n",
    "        \"id\":  author_id,\n",
    "        \"tweet_id\": tweet.get(\"id\", \"\"),\n",
    "        \"text\":     text\n",
    "    })\n",
    "    tweet_counts[author_id] += 1\n",
    "    kept_tweets += 1\n",
    "\n",
    "    # Early stopping\n",
    "    if kept_tweets % 100000 == 0:\n",
    "        # Check progress occasionally\n",
    "        users_with_any = sum(1 for u in selected_user_ids if tweet_counts[u] > 0)\n",
    "        users_full = sum(1 for u in selected_user_ids if tweet_counts[u] >= MAX_TWEETS_PER_USER)\n",
    "        print(f\"Processed {total_lines} lines, kept {kept_tweets} tweets | \"\n",
    "              f\"users with any tweets: {users_with_any}, full: {users_full}\")\n",
    "\n",
    "        # Early stopping condition: if ALL selected users reached max_capped tweets\n",
    "        if users_full == len(selected_user_ids):\n",
    "            print(\"All selected users reached the max tweet cap. Stopping early.\")\n",
    "            break\n",
    "\n",
    "print(\"\\nDone straming.\")\n",
    "print(\"Total lines scanned:\", total_lines)\n",
    "print(\"Total tweets kept:\", kept_tweets)\n",
    "print(\"Distinct users with tweets:\", len(tweet_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71145542-9447-4efd-a467-a81a2b5c59e9",
   "metadata": {},
   "source": [
    "## Section 7: Building posts.csv and filter users by `MIN_TWEETs_PER_USER`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368bcbbd-178e-4d88-b94c-6215a01c1899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Building posts DataFrame and filter users --- #\n",
    "posts_df = pd.DataFrame(tweets_records)\n",
    "print(\"Raw posts_df shape:\", posts_df.shape)\n",
    "display(posts_df.head())\n",
    "\n",
    "space()\n",
    "\n",
    "# --- Counting tweets per user --- #\n",
    "tweet_counts_series = posts_df['id'].value_counts()\n",
    "print(\"\\nTweets per user (summary):\")\n",
    "display(tweet_counts_series.describe())\n",
    "\n",
    "space()\n",
    "\n",
    "# --- Keeping only users with at least MIN_TWEETS_PER_USER tweets --- #\n",
    "eligible_user_ids = set(tweet_counts_series[tweet_counts_series >= MIN_TWEETS_PER_USER].index)\n",
    "print(f\"\\nUsers with at least {MIN_TWEETS_PER_USER} tweets:\", len(eligible_user_ids))\n",
    "\n",
    "space()\n",
    "\n",
    "# --- Filtering posts --- #\n",
    "posts_df = posts_df[posts_df['id'].isin(eligible_user_ids)].reset_index(drop=True)\n",
    "print(\"Filtered posts_df shape:\", posts_df.shape)\n",
    "\n",
    "# --- Filtering user metadata to those with sufficient tweets --- #\n",
    "final_users = selected_users[selected_users[\"id\"].isin(eligible_user_ids)].copy()\n",
    "print(\"Final users count:\", len(final_users))\n",
    "print(final_users[\"label\"].value_counts())\n",
    "\n",
    "space()\n",
    "\n",
    "display(final_users.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fb8b5c-d4c7-49da-acb5-fe96a0c17815",
   "metadata": {},
   "source": [
    "## Section 8: Downloading profile images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af60083-abb3-4cbe-9c00-781012efc10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_image(url, save_path):\n",
    "    \"\"\"Downloading an image from url with error handling\"\"\"\n",
    "    try:\n",
    "        resp = requests.get(url, timeout=5)\n",
    "        if resp.status_code == 200:\n",
    "            img = Image.open(BytesIO(resp.content)).convert(\"RGB\")\n",
    "            img.save(save_path, format=\"JPEG\")\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "# --- Downloading images for all final users --- #\n",
    "local_paths = []\n",
    "\n",
    "print(\"Downloading TwiBot profile images...\")\n",
    "\n",
    "for _, row in tqdm(final_users.iterrows(), total=len(final_users)):\n",
    "    user_id = row[\"id\"]\n",
    "    url = row.get(\"profile_image_url\", None)\n",
    "\n",
    "    if url is None or not isinstance(url, str):\n",
    "        local_paths.append(None)\n",
    "        continue\n",
    "\n",
    "    save_path = IMAGE_DIR / f\"{user_id}.jpg\"\n",
    "    \n",
    "    if Path.exists(save_path):     # Skipping if image is already downloaded\n",
    "        local_paths.append(str(save_path))\n",
    "        continue\n",
    "\n",
    "    # Trying to download\n",
    "    ok = download_image(url, save_path)\n",
    "    if ok:\n",
    "        local_paths.append(str(save_path))\n",
    "    else:\n",
    "        local_paths.append(None)\n",
    "\n",
    "print(\"Download Completed.\")\n",
    "\n",
    "# --- Adding resolved local image paths --- #\n",
    "final_users[\"profile_image_path\"] = local_paths\n",
    "\n",
    "# --- Removing the original URL column (Optional, can be commented) --- #\n",
    "final_users = final_users.drop(columns=[\"profile_image_url\"], errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbf2159-abfb-42e9-a47b-4e718962e16d",
   "metadata": {},
   "source": [
    "## Section 9: Saving cleaned users and posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106d3439-51f9-4b9f-a7c1-ec86b9e65358",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_out_path = PROCESSED_DIR / \"users.csv\"\n",
    "posts_out_path = PROCESSED_DIR / \"posts.csv\"\n",
    "\n",
    "final_users.to_csv(str(users_out_path), index=False)\n",
    "posts_df.to_csv(str(posts_out_path), index=False)\n",
    "\n",
    "print(\"Saved users to:\", users_out_path)\n",
    "print(\"Saved posts to:\", posts_out_path)\n",
    "\n",
    "space()\n",
    "\n",
    "print(\"\\nFinal summary:\")\n",
    "print(\"Users:\", len(final_users))\n",
    "print(\"Posts:\", len(posts_df))\n",
    "print(\"Label distribution (users):\")\n",
    "print(final_users[\"label\"].value_counts())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
