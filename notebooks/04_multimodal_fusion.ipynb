{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d441307e-3bcf-498c-b235-3b304d42ba28",
   "metadata": {},
   "source": [
    "# Multimodal Fusion: Text + Image\n",
    "This notebook builds a **late fusion classifier** that integrates three complementary signals:\n",
    "- **RoBERTa text classifier** -> Produces a probability score for \"bot\"\n",
    "- **CLIP image classifier** -> probability that the image looks like a bot profile\n",
    "- **image_exists_flag** -> binary feature indicating whether the user even has a profile image\n",
    "- **CLIP image-text alignment score** -> cosine similarity between a user's profile image and their textual identity\n",
    "\n",
    "**What do we do here:**\n",
    "\n",
    "1. **Load all three features per user**\n",
    "   - `text_prob` from RoBERTa\n",
    "   - `img_prob` from CLIP\n",
    "   - `has_image` as a structural / social cue\n",
    "   - `clip_img_text_sim` as a semantic alignment score  \n",
    "     Combine them into a single feature matrix for each user.\n",
    "\n",
    "<br> \n",
    "\n",
    "2. **Split users into train / validation / test**\n",
    "   - Same split as the RoBERTa notebook to keep evaluation consistent.\n",
    "\n",
    "<br> \n",
    "\n",
    "3. **Train fusion models**\n",
    "   - Train an **XGBoost Classifier**, which handles nonlinear interactions well.\n",
    "   - Optional: hyperparameter tuning via randomized search.\n",
    "\n",
    "<br> \n",
    "\n",
    "4. **Evaluate fusion performance**\n",
    "   - Accuracy, percision, recall, f1-score, AUROC.\n",
    "   - Compare against text-only and image-only models.\n",
    "   - Plot a confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c5e172-2035-43fd-aa3a-6a25d51cff4a",
   "metadata": {},
   "source": [
    "## Section 1: Imports & Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7295345f-98c5-4b57-a41d-b8de8aa6dd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------------#\n",
    "# HuggingFace Cache Location                                                      #\n",
    "#---------------------------------------------------------------------------------#\n",
    "# By default, HuggingFace downloads pretrained models into the user directory     #\n",
    "# (e.g., ~/.cache/huggingface/). To make the project fully reproducible and       #\n",
    "# avoid polluting the user's  global cache, we redirect HF_HOME to a local        #\n",
    "# folder inside the project.                                                      #\n",
    "#                                                                                 #\n",
    "# If you prefer a different cache directory, simply modify HF_CACHE below.        #\n",
    "# If the folder does not exist yet, HuggingFace will create it automatically.     #\n",
    "#---------------------------------------------------------------------------------#\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path.cwd().parent.resolve()\n",
    "HF_CACHE = ROOT / \"hf_cache\"\n",
    "os.environ[\"HF_HOME\"] = str(HF_CACHE)\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import CLIPProcessor, CLIPModel, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --- Paths and Configurations --- #\n",
    "TWIBOT_DIR     = ROOT / \"data/twibot22/processed\"\n",
    "USERS_PATH     = TWIBOT_DIR / \"users_with_clip_sim.csv\"\n",
    "POSTS_PATH     = TWIBOT_DIR / \"posts.csv\"\n",
    "TEXT_MODEL_DIR = ROOT / \"outputs/roberta_twibot22/roberta_twibot22_model\"\n",
    "OUTPUT_DIR     = ROOT / \"outputs/multimodal\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SEED               = 42\n",
    "BATCH_SIZE_IMG     = 32\n",
    "MAX_SEQ_LENGTH     = 256\n",
    "\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device.type == \"cuda\":\n",
    "    print(f\"Using cuda on {torch.cuda.get_device_name()}\")\n",
    "else:\n",
    "    print(\"Using cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4eeca7e-7501-454b-8afa-ec12e152b99a",
   "metadata": {},
   "source": [
    "## Section 2: Loading users with both text + image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adeb4f34-68de-4b33-aa9d-a6ebb94dba34",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_mm = pd.read_csv(USERS_PATH)\n",
    "\n",
    "# --- Balancing human/bot distribution --- #\n",
    "humans = users_mm[users_mm[\"label_num\"] == 0]\n",
    "bots = users_mm[users_mm[\"label_num\"] == 1]\n",
    "\n",
    "n = min(len(humans), len(bots))\n",
    "humans_bal = humans.sample(n, random_state=SEED)\n",
    "bots_bal = bots.sample(n, random_state=SEED)\n",
    "\n",
    "data_bal = pd.concat([humans_bal, bots_bal]).sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "users_mm = data_bal.copy().reset_index(drop=True)\n",
    "\n",
    "print(\"\\nUsers in multimodal set:\", users_mm.shape)\n",
    "print(\"Label distribution (After balancing):\")\n",
    "print(users_mm[\"label_num\"].value_counts())\n",
    "\n",
    "print(\"\\nImage availability by class (After Balancing):\")\n",
    "print(pd.crosstab(users_mm[\"label_num\"], users_mm[\"image_exists\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b8a952-2c84-4759-ae30-74062f3d9ecc",
   "metadata": {},
   "source": [
    "## Section 3: Train / Val / Test split (shared for text and image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70201980-1ace-43a0-9573-0c6bac214226",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_df, test_df = train_test_split(\n",
    "    users_mm,\n",
    "    test_size=0.2,\n",
    "    stratify=users_mm[\"label_num\"],\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "train_df, val_df = train_test_split(\n",
    "    train_val_df,\n",
    "    test_size=0.2,\n",
    "    stratify=train_val_df[\"label_num\"],\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "print(\"Train users:\", len(train_df))\n",
    "print(\"Val users:\",   len(val_df))\n",
    "print(\"Test users:\",  len(test_df))\n",
    "\n",
    "for name, df in [(\"Train\", train_df), (\"Val\", val_df), (\"Test\", test_df)]:\n",
    "    print(f\"\\n{name} label distribution:\")\n",
    "    print(df[\"label_num\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743eeeb6-b424-4035-bcd1-f83086a4e591",
   "metadata": {},
   "source": [
    "## Section 4: Loading RoBERTa model + tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370d6a16-2017-4cf4-88a8-54ed0f55d8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer  = AutoTokenizer.from_pretrained(\"roberta-base\")\n",
    "text_model = AutoModelForSequenceClassification.from_pretrained(TEXT_MODEL_DIR)\n",
    "text_model.to(device)\n",
    "text_model.eval()\n",
    "\n",
    "print(\"Loaded text model from:\", TEXT_MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5591a6f8-4802-48bb-b580-8fbfe9b10d36",
   "metadata": {},
   "source": [
    "## Section 5: Text dataset + helper to compute probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edbd164-cd2f-4f76-936c-c23cd756f7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserTextDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, max_len):\n",
    "        self.texts  = df[\"full_text\"].astype(str).tolist()\n",
    "        self.labels = df[\"label_num\"].astype(int).tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text  = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        enc = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        item = {k: v.squeeze(0) for k, v in enc.items()}\n",
    "        item[\"labels\"] = torch.tensor(label, dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "def get_text_probs(df):\n",
    "    dataset = UserTextDataset(df, tokenizer=tokenizer, max_len=MAX_SEQ_LENGTH)\n",
    "    loader = DataLoader(dataset=dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "    all_probs  = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            labels = batch[\"labels\"].numpy()\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "            outputs = text_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits  # [B, 2]\n",
    "            probs = torch.softmax(logits, dim=-1)[:, 1] # prob of class 1 (bot)\n",
    "            all_probs.append(probs.cpu().numpy())\n",
    "            all_labels.append(labels)\n",
    "\n",
    "    return np.concatenate(all_probs), np.concatenate(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58982e40-4d1b-482d-bb39-2c833523bbef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# --- Computing text-based bot probabilities --- #\n",
    "\n",
    "train_text_prob, train_labels = get_text_probs(train_df)\n",
    "val_text_prob, val_labels = get_text_probs(val_df)\n",
    "test_text_prob, test_labels = get_text_probs(test_df)\n",
    "\n",
    "print(\"Text probs shapes:\", train_text_prob, val_text_prob, test_text_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08379fa-dc06-4429-8649-ba57cfcff1a6",
   "metadata": {},
   "source": [
    "## Section 6: CLIP image embeddings and image-based probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595d5bda-823a-4cc5-b079-44cb42d847b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "clip_model.to(device)\n",
    "clip_model.eval()\n",
    "\n",
    "print(\"CLIP loaded on:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75284d7-7583-4051-97ea-1a4b3e18b540",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwibotImageDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.paths  = df[\"profile_image_path\"].tolist()\n",
    "        self.labels = df[\"label_num\"].astype(int).tolist()\n",
    "        self.ids    = df[\"id\"].tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path  = self.paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = Image.open(path).convert(\"RGB\")\n",
    "        uid   = self.ids[idx]\n",
    "        return image, label, uid\n",
    "\n",
    "def collate_pil(batch):\n",
    "    images, labels, ids = zip(*batch)\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    return list(images), labels, list(ids)\n",
    "\n",
    "def extract_clip_features_for_split(df_split):\n",
    "    df_img = df_split[df_split[\"image_exists\"]].copy().reset_index(drop=True)\n",
    "    if df_img.empty:\n",
    "        return None, None, None\n",
    "\n",
    "    dataset = TwibotImageDataset(df_img)\n",
    "    loader = DataLoader(dataset, batch_size=BATCH_SIZE_IMG, shuffle=False, collate_fn=collate_pil)\n",
    "\n",
    "    all_feats  = []\n",
    "    all_labels = []\n",
    "    all_ids    = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels, ids in tqdm(loader, desc=\"CLIP features\"):\n",
    "            inputs = clip_processor(images=images, return_tensors=\"pt\")\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "            outputs = clip_model.get_image_features(**inputs)\n",
    "\n",
    "            feats = outputs.cpu().numpy()\n",
    "            feats = feats / np.linalg.norm(feats, axis=1, keepdims=True)\n",
    "            \n",
    "            all_feats.append(feats)\n",
    "            all_labels.append(labels.numpy())\n",
    "            all_ids.extend(ids)\n",
    "\n",
    "    feats = np.vstack(all_feats)\n",
    "    labels = np.concatenate(all_labels)\n",
    "    ids = np.array(all_ids)\n",
    "\n",
    "    return feats, labels, ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc95317-24f4-496e-9206-0e9e729fe680",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_img_feats, train_img_labels, train_img_ids = extract_clip_features_for_split(train_df)\n",
    "val_img_feats, val_img_labels, val_img_ids = extract_clip_features_for_split(val_df)\n",
    "test_img_feats, test_img_labels, test_img_ids = extract_clip_features_for_split(test_df)\n",
    "\n",
    "print(\"Image feats:\", train_img_feats, val_img_feats, test_img_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e766fa7-e4e0-478f-8ba7-186d986314eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Training a simple image-only classifier, mainly to get image-based bot probabilities\n",
    "\n",
    "scaler_img = StandardScaler(with_mean=False)\n",
    "\n",
    "train_img_scaled = scaler_img.fit_transform(train_img_feats)\n",
    "val_img_scaled   = scaler_img.transform(val_img_feats)\n",
    "test_img_scaled  = scaler_img.transform(test_img_feats)\n",
    "\n",
    "img_clf = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    class_weight=\"balanced\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "img_clf.fit(train_img_scaled, train_img_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58272e9b-95e9-4704-825c-b8f137e1a385",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_img_prob_has = img_clf.predict_proba(train_img_scaled)[:, 1]\n",
    "val_img_prob_has   = img_clf.predict_proba(val_img_scaled)[:, 1]\n",
    "test_img_prob_has  = img_clf.predict_proba(test_img_scaled)[:, 1]\n",
    "\n",
    "train_df[\"img_prob\"] = np.nan\n",
    "val_df[\"img_prob\"]   = np.nan\n",
    "test_df[\"img_prob\"]  = np.nan\n",
    "\n",
    "train_df.loc[train_df[\"id\"].isin(train_img_ids), \"img_prob\"] = train_img_prob_has\n",
    "val_df.loc[val_df[\"id\"].isin(val_img_ids), \"img_prob\"]       = val_img_prob_has\n",
    "test_df.loc[test_df[\"id\"].isin(test_img_ids), \"img_prob\"]    = test_img_prob_has\n",
    "\n",
    "train_img_prob_filled = train_df[\"img_prob\"].fillna(0.5).to_numpy()\n",
    "val_img_prob_filled   = val_df[\"img_prob\"].fillna(0.5).to_numpy()\n",
    "test_img_prob_filled  = test_df[\"img_prob\"].fillna(0.5).to_numpy()\n",
    "\n",
    "train_has_image = train_df[\"image_exists\"].astype(int).to_numpy()\n",
    "val_has_image   = val_df[\"image_exists\"].astype(int).to_numpy()\n",
    "test_has_image  = test_df[\"image_exists\"].astype(int).to_numpy()\n",
    "\n",
    "train_sim = train_df[\"clip_img_text_sim\"].fillna(0.0).to_numpy()\n",
    "val_sim   = val_df[\"clip_img_text_sim\"].fillna(0.0).to_numpy()\n",
    "test_sim  = test_df[\"clip_img_text_sim\"].fillna(0.0).to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3366eed3-60aa-470d-86d0-373a40a4dc53",
   "metadata": {},
   "source": [
    "## Section 7: Building fusion features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931da256-5c5b-4199-9ed9-cef451ace461",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.stack([train_text_prob, train_img_prob_filled, train_has_image, train_sim], axis=1)\n",
    "X_val   = np.stack([val_text_prob,   val_img_prob_filled,   val_has_image,   val_sim],   axis=1)\n",
    "X_test  = np.stack([test_text_prob,  test_img_prob_filled,  test_has_image,  test_sim],  axis=1)\n",
    "\n",
    "y_train = train_df[\"label_num\"].to_numpy()\n",
    "y_val   = val_df[\"label_num\"].to_numpy()\n",
    "y_test  = test_df[\"label_num\"].to_numpy()\n",
    "\n",
    "print(\"Fusion feature shapes:\", X_train.shape, X_val.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3a8e31-d3db-43f5-8b68-9183c9404eb7",
   "metadata": {},
   "source": [
    "## Section 8: Training fusion classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d24cb54-2123-4e83-bcef-392b5dd4aff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = (y_train == 1).sum()\n",
    "neg = (y_train == 0).sum()\n",
    "scale_pos_weight = neg / pos if pos > 0 else 1.0\n",
    "print(\"Train pos/neg:\", pos, neg, \"scale_pos_weight:\", scale_pos_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29143b29-c2b1-4296-9fcc-2ddefc656fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "param_grid = {\n",
    "    \"n_estimators\"    : [50, 100, 200, 400],\n",
    "    \"max_depth\"       : [2, 3, 4, 5],\n",
    "    \"learning_rate\"   : [0.005, 0.01, 0.03],\n",
    "    \"subsample\"       : [0.005, 0.01, 0.05, 0.2, 0.5, 1.0],\n",
    "    \"colsample_bytree\": [0.005, 0.01, 0.05, 0.2, 0.5, 1.0]\n",
    "}\n",
    "\n",
    "best_auc    = -1.0\n",
    "best_params = None\n",
    "results     = []\n",
    "\n",
    "for n_estimators in param_grid[\"n_estimators\"]:\n",
    "    for max_depth in param_grid[\"max_depth\"]:\n",
    "        for learning_rate in param_grid[\"learning_rate\"]:\n",
    "            for subsample in param_grid[\"subsample\"]:\n",
    "                for colsample_bytree in param_grid[\"colsample_bytree\"]:\n",
    "                    xgb = XGBClassifier(\n",
    "                        n_estimators=n_estimators,\n",
    "                        max_depth=max_depth,\n",
    "                        learning_rate=learning_rate,\n",
    "                        subsample=subsample,\n",
    "                        colsample_bytree=colsample_bytree,\n",
    "                        reg_lambda=1.0,\n",
    "                        objective=\"binary:logistic\",\n",
    "                        eval_metric=\"logloss\",\n",
    "                        scale_pos_weight=scale_pos_weight,\n",
    "                        random_state=42,\n",
    "                        n_jobs=-1,\n",
    "                        tree_method=\"hist\",   # usually fastest\n",
    "                    )\n",
    "\n",
    "                    xgb.fit(X_train, y_train)\n",
    "\n",
    "                    val_proba = xgb.predict_proba(X_val)[:, 1]\n",
    "                    auc = roc_auc_score(y_val, val_proba)\n",
    "\n",
    "                    results.append((auc, {\n",
    "                        \"n_estimators\": n_estimators,\n",
    "                        \"max_depth\": max_depth,\n",
    "                        \"learning_rate\": learning_rate,\n",
    "                        \"subsample\": subsample,\n",
    "                        \"colsample_bytree\": colsample_bytree,\n",
    "                    }))\n",
    "\n",
    "                    if auc > best_auc:\n",
    "                        best_auc = auc\n",
    "                        best_params = results[-1][1]\n",
    "\n",
    "print(\"Best val AUROC:\", best_auc)\n",
    "print(\"Best params:\", best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566697f6-5c58-42b4-a013-cdcb328d2f37",
   "metadata": {},
   "source": [
    "## Section 9: Final test evaluation (fusion) + Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307d9efd-a007-43cd-a15b-09b98faada1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_fusion = XGBClassifier(\n",
    "    **best_params,\n",
    "    reg_lambda=1.0,\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"logloss\",\n",
    "    scale_pos_weight=scale_pos_weight,\n",
    "    random_state=SEED,\n",
    "    n_jobs=-1,\n",
    "    tree_method=\"hist\"\n",
    ")\n",
    "\n",
    "xgb_fusion.fit(X_train, y_train)\n",
    "test_proba = xgb_fusion.predict_proba(X_test)[:, 1]\n",
    "test_pred  = (test_proba >= 0.5).astype(int)\n",
    "\n",
    "print(\"XGBoost Fusion -- test performance:\")\n",
    "print(classification_report(y_test, test_pred, digits=3, zero_division=0))\n",
    "print(\"XGBoost Fusion -- test AUROC:\", roc_auc_score(y_test, test_proba))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f04749-40e3-46e7-aa5c-4ba521723164",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, test_pred)\n",
    "plt.figure(figsize=(5, 4))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "           xticklabels=[\"human\", \"bot\"],\n",
    "           yticklabels=[\"human\",\"bot\"])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Multimodal Fusion - Confusion Matrix (Test)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(str(OUTPUT_DIR / \"fusion_confusion_matrix_test.png\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bc4697-9394-46a2-a132-0379f26dda63",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = [\"text_prob\", \"img_prob\", \"has_image\", \"clip_img_text_sim\"]\n",
    "importances = xgb_fusion.feature_importances_\n",
    "\n",
    "for name, imp in sorted(zip(feature_names, importances), key=lambda x: -x[1]):\n",
    "    print(f\"{name}: {imp:.3f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
