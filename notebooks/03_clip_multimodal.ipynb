{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90882d8a-b48e-4353-8c03-3155766c8704",
   "metadata": {},
   "source": [
    "# CLIP Feature Extraction\n",
    "In this notebook, we extract **image embeddings** from user's profile pictures using **CLIP (ViT-B/32)**, and evaluate how much information profile images contain about whether an account is a bot.  \n",
    "In addition, we compute an optional semantic alignment score between each user's **profile image** and their **bio+posts**.  \n",
    "This notebook forms the vision branch of the final multimodal bot-detection system.\n",
    "\n",
    "**What we do here:**\n",
    "\n",
    "1. Loading cleaned users.csv\n",
    "These come from the TwiBot-22 preprocessing notebook and include:\n",
    "   - Local filesystem paths to downloaded user profile images\n",
    "   - User bios and aggregated posts (text)\n",
    "   - Bot/human labels\n",
    "   - A flag indicating whether a valid profile image exists\n",
    "\n",
    "<br>\n",
    "\n",
    "2. Loading the CLIP model and processor\n",
    "   - We use `openai/clip-vit-base-patch32`.\n",
    "   - Images are preprocessed into 224x224 tensors.\n",
    "   - Both modalities are encoded into **512-dimensional** embeddings.\n",
    "\n",
    "<br>\n",
    "\n",
    "4. Train a simple classifier\n",
    "For *all* users (including those with missing images):\n",
    "   1. Construct **full_text** = *bio* + *posts*\n",
    "   2. Encode full_text using CLIP's**text encoder**\n",
    "   3. Encode profile images using the vision encoder, when available\n",
    "   4. Compute cosine similarity between image and text embeddings\n",
    "   5. Store the result in `clip_img_text_sim`\n",
    "Users without an image recieve `NaN`, later filled with `0.0` in fusion.\n",
    "This score approximates whether *the profile image semantically matches the account's textual identity*.\n",
    "\n",
    "<br>\n",
    "\n",
    "5. Save the results for the Multimodal Fusion notebook\n",
    "We export `users_with_clip_sim.csv` which contains:\n",
    "    - id\n",
    "    - full_text\n",
    "    - image_exists\n",
    "    - profile_image_path\n",
    "    - label_num\n",
    "    - clip_img_text_sim\n",
    "\n",
    "<br>\n",
    "\n",
    "6. Extracting pure image embeddings\n",
    "For users that have a profile image:\n",
    "   - Create a PyTorch dataloader\n",
    "   - Feed the images through CLIP's vision encoder\n",
    "   - Normalize and store the 512-dimensional feature vectors\n",
    "   - Train a simple logistic regression image classifier\n",
    "   - Evaluate accuracy, F1, and AUROC on image-only prediction\n",
    "This gives us an estimate of how useful profile pictures alone are for bot detection.\n",
    "\n",
    "<br>\n",
    "\n",
    "7. Evaluate image-only performance\n",
    "   - Accuracy, macro F1, AUROC."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8454a2-4f7e-45fb-87f8-e2dc50a59e2c",
   "metadata": {},
   "source": [
    "## Section 1: Imports and Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d26fd00-2ebb-4720-abd6-e7f05deecb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------------#\n",
    "# HuggingFace Cache Location                                                      #\n",
    "#---------------------------------------------------------------------------------#\n",
    "# By default, HuggingFace downloads pretrained models into the user directory     #\n",
    "# (e.g., ~/.cache/huggingface/). To make the project fully reproducible and       #\n",
    "# avoid polluting the user's  global cache, we redirect HF_HOME to a local        #\n",
    "# folder inside the project.                                                      #\n",
    "#                                                                                 #\n",
    "# If you prefer a different cache directory, simply modify HF_CACHE below.        #\n",
    "# If the folder does not exist yet, HuggingFace will create it automatically.     #\n",
    "#---------------------------------------------------------------------------------#\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path.cwd().parent.resolve()\n",
    "HF_CACHE = ROOT / \"hf_cache\"\n",
    "os.environ[\"HF_HOME\"] = str(HF_CACHE)\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# --- Paths and Configurations --- #\n",
    "TWIBOT_PATH = ROOT / \"data/twibot22\"\n",
    "USERS_PATH  = TWIBOT_PATH / \"processed/users.csv\"\n",
    "POSTS_PATH  = TWIBOT_PATH / \"processed/posts.csv\"\n",
    "OUTPUT_DIR  = ROOT / \"outputs/clip_twibot22\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SEED = 42\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device.type == \"cuda\":\n",
    "    print(f\"Using cuda on {torch.cuda.get_device_name()}\")\n",
    "else:\n",
    "    print(\"Using cpu\")\n",
    "\n",
    "def space():\n",
    "    print(\"\\n\" ,\"-\" * 100, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a210d5d-d1c4-49a7-90a9-e25c6f0b387e",
   "metadata": {},
   "source": [
    "## Section 2: Loading TwiBot users and with images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89204ffc-6551-49ce-8d91-b09e066b0a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_df = pd.read_csv(str(USERS_PATH))\n",
    "posts_df = pd.read_csv(str(POSTS_PATH))\n",
    "print(\"Users shape:\", users_df.shape)\n",
    "print(users_df.head())\n",
    "\n",
    "space()\n",
    "\n",
    "users_df[\"profile_image_path\"] = users_df[\"profile_image_path\"].astype(str)\n",
    "users_df[\"image_exists\"] = users_df[\"profile_image_path\"].apply(lambda p: Path(p).exists())\n",
    "#users_df = users_df[users_df[\"image_exists\"]].copy().reset_index(drop=True)\n",
    "\n",
    "# --- Aggregating tweets per user (concatenated text) --- #\n",
    "posts_agg = (\n",
    "    posts_df.groupby(\"id\")[\"text\"]\n",
    "    .apply(lambda x: \" \".join(x.astype(str)))\n",
    "    .reset_index()\n",
    "    .rename(columns={\"text\": \"tweets_text\"})\n",
    ")\n",
    "\n",
    "# --- Merging into users --- #\n",
    "data_df = users_df.merge(posts_agg, on=\"id\", how=\"left\")\n",
    "data_df[\"tweets_text\"] = data_df[\"tweets_text\"].fillna(\"\")\n",
    "\n",
    "# --- Explicit Bio text --- #\n",
    "data_df[\"bio_text\"] = data_df[\"description\"].fillna(\"\")\n",
    "\n",
    "MAX_TWEET_CHARS = 6000\n",
    "data_df[\"tweets_text\"] = data_df[\"tweets_text\"].str.slice(0, MAX_TWEET_CHARS)\n",
    "\n",
    "# --- Full text = description + tweets --- #\n",
    "data_df[\"full_text\"] = (\n",
    "    \"Bio: \" + data_df[\"bio_text\"] + \" Posts: \" + data_df[\"tweets_text\"]\n",
    ").str.strip()\n",
    "\n",
    "# --- Attaching image info --- #\n",
    "data_df[\"profile_image_path\"] = data_df[\"profile_image_path\"].astype(str)\n",
    "data_df[\"image_exists\"] = data_df[\"profile_image_path\"].apply(lambda p: Path(p).exists())\n",
    "\n",
    "# --- Length stats --- #\n",
    "data_df[\"char_length\"] = data_df[\"full_text\"].apply(len)\n",
    "print(\"\\nDoc length summary (chars):\")\n",
    "display(data_df[\"char_length\"].describe())\n",
    "\n",
    "space()\n",
    "\n",
    "# --- Filtering out users with every little text --- #\n",
    "min_char = 50\n",
    "data_df = data_df[data_df[\"char_length\"] >= min_char].reset_index(drop=True)\n",
    "print(\"\\nUsers after min_char filter:\", len(data_df))\n",
    "print(data_df[\"label\"].value_counts())\n",
    "\n",
    "space()\n",
    "\n",
    "print(\"\\nData with full_text & image flag:\", data_df.shape)\n",
    "print(data_df[[\"id\", \"label_num\", \"image_exists\"]].head())\n",
    "\n",
    "space()\n",
    "\n",
    "print(\"Label distribution (Before balancing):\")\n",
    "print(data_df[\"label_num\"].value_counts())\n",
    "print(\"\\nImage availability by class (before balancing):\")\n",
    "print(pd.crosstab(data_df[\"label_num\"], data_df[\"image_exists\"]))\n",
    "\n",
    "space()\n",
    "\n",
    "users_mm = data_df[[\"id\", \"full_text\", \"image_exists\", \"profile_image_path\", \"label_num\"]].copy()\n",
    "print(\"Final Users shape:\", users_mm.shape)\n",
    "print(users_mm.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8b929e-147a-40e5-9692-bef0a4f0daba",
   "metadata": {},
   "source": [
    "## Section 3: Loading CLIP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97eca99a-822b-45ee-97c8-81089abf1923",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"openai/clip-vit-base-patch32\"\n",
    "\n",
    "clip_model = CLIPModel.from_pretrained(MODEL_NAME).to(device)\n",
    "clip_processor = CLIPProcessor.from_pretrained(MODEL_NAME)\n",
    "\n",
    "clip_model.to(device)\n",
    "clip_model.eval()\n",
    "\n",
    "print(\"CLIP loaded on:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589ec030-6874-41f0-85a1-542ed2001323",
   "metadata": {},
   "source": [
    "## Section 4: Computing CLIP similarities & Exporting the final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd12c5c9-7e7c-4d50-aff9-e7aa60074deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_sims = []\n",
    "\n",
    "for _, row in tqdm(users_mm.iterrows(), total=len(users_mm), desc=\"Computing CLIP image-text similarity\"):\n",
    "    text = str(row[\"full_text\"])\n",
    "    img_path = row[\"profile_image_path\"]\n",
    "\n",
    "    sim = np.nan\n",
    "\n",
    "    try:\n",
    "        if isinstance(img_path, str) and Path(img_path).exists():\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "    \n",
    "            inputs = clip_processor(\n",
    "                text=[text],\n",
    "                images=image,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=77\n",
    "            ).to(device)\n",
    "    \n",
    "            with torch.no_grad():\n",
    "                outputs = clip_model(**inputs)\n",
    "                img_emb = outputs.image_embeds\n",
    "                txt_emb = outputs.text_embeds\n",
    "    \n",
    "            img_emb = img_emb / img_emb.norm(dim=-1, keepdim=True)\n",
    "            txt_emb = txt_emb / txt_emb.norm(dim=-1, keepdim=True)\n",
    "            sim = (img_emb * txt_emb).sum(dim=-1).item()\n",
    "        else:\n",
    "            sim = np.nan\n",
    "\n",
    "    except Exception:\n",
    "        sim = np.nan\n",
    "\n",
    "    clip_sims.append(sim)\n",
    "\n",
    "users_mm.loc[:, \"clip_img_text_sim\"] = clip_sims\n",
    "\n",
    "space()\n",
    "\n",
    "display(users_mm.head())\n",
    "\n",
    "space()\n",
    "\n",
    "OUT_PATH = TWIBOT_PATH / \"processed/users_with_clip_sim.csv\"\n",
    "users_mm.to_csv(str(OUT_PATH), index=False)\n",
    "print(\"Saved:\", OUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5573e06-85dd-4820-891c-84895e414c1a",
   "metadata": {},
   "source": [
    "## Section 5: Train / Val / Test split (user-level, stratified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1ebb36-c349-4d35-9ef1-85b9edaff428",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_img = users_mm[users_mm[\"image_exists\"]].copy().reset_index(drop=True)\n",
    "\n",
    "# --- Balancing human/bot distribution --- #\n",
    "humans = users_img[users_img[\"label_num\"] == 0]\n",
    "bots = users_img[users_img[\"label_num\"] == 1]\n",
    "\n",
    "n = min(len(humans), len(bots))\n",
    "humans_bal = humans.sample(n, random_state=SEED)\n",
    "bots_bal = bots.sample(n, random_state=SEED)\n",
    "\n",
    "data_bal = pd.concat([humans_bal, bots_bal]).sample(frac=1, random_state=SEED).reset_index(drop=True)\n",
    "users_img = data_bal.copy().reset_index(drop=True)\n",
    "\n",
    "train_val_df, test_df = train_test_split(\n",
    "    users_img,\n",
    "    test_size=0.2,\n",
    "    stratify=users_img[\"label_num\"],\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "train_df, val_df = train_test_split(\n",
    "    train_val_df,\n",
    "    test_size=0.2,\n",
    "    stratify=train_val_df[\"label_num\"],\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "print(\"Train users:\", len(train_df))\n",
    "print(\"Val users:\", len(val_df))\n",
    "print(\"Test users:\", len(test_df))\n",
    "\n",
    "space()\n",
    "\n",
    "print(\"\\nTrain label distribution:\")\n",
    "print(train_df[\"label_num\"].value_counts())\n",
    "space()\n",
    "print(\"\\nVal label distribution:\")\n",
    "print(val_df[\"label_num\"].value_counts())\n",
    "space()\n",
    "print(\"\\nTest label distribution:\")\n",
    "print(test_df[\"label_num\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c8c57a-0b31-46c1-915b-8606a28ad70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwibotImageDataset(Dataset):\n",
    "    def __init__(self, df, processor):\n",
    "        self.paths = df[\"profile_image_path\"].tolist()\n",
    "        self.labels = df[\"label_num\"].astype(int).tolist()\n",
    "        self.processor = processor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = self.paths[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        image = Image.open(path).convert(\"RGB\")\n",
    "        return image, label\n",
    "\n",
    "train_dataset = TwibotImageDataset(train_df, clip_processor)\n",
    "val_dataset   = TwibotImageDataset(val_df, clip_processor)\n",
    "test_dataset  = TwibotImageDataset(test_df, clip_processor)\n",
    "\n",
    "print(\"Train size:\", len(train_dataset))\n",
    "print(\"Val size:\", len(val_dataset))\n",
    "print(\"Test size:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc7a61b-f014-4c53-b7b3-8a2b2c4b1326",
   "metadata": {},
   "source": [
    "## Section 6: Helper -> Extracts CLIP image embeddings\n",
    "Using this helper, we run CLIP once over the images and cache the 512-dim image features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9067b80a-aa1d-4bfd-9e39-2fe3ff4b1a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    images, labels = zip(*batch)\n",
    "    return list(images), torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "def extract_clip_image_features(dataset, batch_size=32):\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "    all_feats = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(dataloader, desc=\"Extracting CLIP features\"):\n",
    "            inputs = clip_processor(images=images, return_tensors=\"pt\").to(device)\n",
    "            outputs = clip_model.get_image_features(**inputs)\n",
    "            feats = outputs.detach().cpu().numpy()\n",
    "\n",
    "            feats = feats / np.linalg.norm(feats, axis=1, keepdims=True)\n",
    "            all_feats.append(feats)\n",
    "            all_labels.append(labels.numpy())\n",
    "\n",
    "    all_feats = np.vstack(all_feats)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    return all_feats, all_labels\n",
    "\n",
    "train_feats, train_labels = extract_clip_image_features(train_dataset, batch_size=BATCH_SIZE)\n",
    "val_feats, val_labels     = extract_clip_image_features(val_dataset, batch_size=BATCH_SIZE)\n",
    "test_feats, test_labels   = extract_clip_image_features(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "print(\"Train feats:\", train_feats.shape)\n",
    "print(\"Val feats:\", val_feats.shape)\n",
    "print(\"Test feats:\", test_feats.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95d1974-32bc-4bde-ba06-8a7a3a675883",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez(\n",
    "    OUTPUT_DIR / \"twibot_clip_image_features.npz\",\n",
    "    train_feats=train_feats, train_labels=train_labels,\n",
    "    val_feats=val_feats,     val_labels=val_labels,\n",
    "    test_feats=test_feats,   test_labels=test_labels\n",
    ")\n",
    "\n",
    "print(\"Saved CLIP features.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48e611d-dcbb-458f-ab83-b877b7b58aad",
   "metadata": {},
   "source": [
    "## Section 7: Training a simple classifier (logistic regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66a9154-0c31-4cdd-828c-4d340d41afdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(with_mean=False) # Because they are already been normalized\n",
    "train_feats_scaled = scaler.fit_transform(train_feats)\n",
    "val_feats_scaled = scaler.fit_transform(val_feats)\n",
    "test_feats_scaled = scaler.fit_transform(test_feats)\n",
    "\n",
    "clf = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    class_weight=\"balanced\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "clf.fit(train_feats_scaled, train_labels)\n",
    "\n",
    "print(\"Validation performance (image-only CLIP):\")\n",
    "val_pred = clf.predict(val_feats_scaled)\n",
    "print(classification_report(val_labels, val_pred, digits=3, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83542402-df04-4d31-8876-2a1e6e6309a5",
   "metadata": {},
   "source": [
    "## Section 8: Evaluating on Test set + Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a14c25-af47-4409-8abf-8a4e0c62583c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = clf.predict(test_feats_scaled)\n",
    "test_proba = clf.predict_proba(test_feats_scaled)[:, 1]\n",
    "\n",
    "print(\"Test performance (image-only CLIP):\")\n",
    "print(classification_report(test_labels, test_pred, digits=3, zero_division=0))\n",
    "\n",
    "try:\n",
    "    auroc = roc_auc_score(test_labels, test_proba)\n",
    "    print(\"Test AUROC:\", auroc)\n",
    "except Exception as e:\n",
    "    print(\"Could not compute AUROC:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2902b302-9318-4124-a253-02129a97b5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(test_labels, test_pred)\n",
    "plt.figure(figsize=(5, 4))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "           xticklabels=[\"human\", \"bot\"],\n",
    "           yticklabels=[\"human\",\"bot\"])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"TwiBot-22 Image-only CLIP - Confusion Matrix (Test)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(str(OUTPUT_DIR / \"confusion_matrix_test.png\"))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
